{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# IO Backend Performance\n\nLeverage different IO backends for storing inference results.\n\nThis example explores IO backends inside Earth2Studio and how they can be used to write\ndata to different formats / locations. The IO is a core part of any inference pipeline\nand depending on the desired target, can dramatically impact performance. This example\nwill help navigate users through the use of different IO backend APIs in a simple\nworkflow.\n\nIn this example you will learn:\n\n- Initializing, creating arrays and writing with the Zarr IO backend\n- Initializing, creating arrays and writing with the NetCDF IO backend\n- Initializing and writing with the Asynchronous Non-blocking Zarr IO backend\n- Discussing performance implications and strategies that can be used\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up\nTo demonstrate different IO, this example will use a simple ensemble workflow that we\nwill manually create ourselves. One could use the built in workflow in Earth2Studio\nhowever, this will allow us to better understand the APIs.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need the following components:\n\n- Datasource: Pull data from the GFS data api :py:class:`earth2studio.data.GFS`.\n- Prognostic Model: Use the built in DLWP model :py:class:`earth2studio.models.px.DLWP`.\n- Perturbation Method: Use the standard Gaussian method :py:class:`earth2studio.perturbation.Gaussian`.\n- IO Backends: Use a few IO Backends including :py:class:`earth2studio.io.AsyncZarrBackend`, :py:class:`earth2studio.io.NetCDF4Backend` and :py:class:`earth2studio.io.ZarrBackend`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nos.makedirs(\"outputs\", exist_ok=True)\nfrom dotenv import load_dotenv\n\nload_dotenv()  # TODO: make common example prep function\n\nimport torch\n\nfrom earth2studio.data import GFS, DataSource, fetch_data\nfrom earth2studio.io import AsyncZarrBackend, IOBackend, NetCDF4Backend, ZarrBackend\nfrom earth2studio.models.px import DLWP, PrognosticModel\nfrom earth2studio.perturbation import Gaussian, Perturbation\n\n# Get the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the cBottle data source\npackage = DLWP.load_default_package()\nmodel = DLWP.load_model(package)\nmodel = model.to(device)\n\n# Create the ERA5 data source\nds = GFS()\n\n# Create perturbation method\npt = Gaussian()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Simple Ensemble Workflow\nStart with creating a simple ensemble inference workflow. This is essentially a\nsimpler version of the built in ensemble workflow :py:meth:`earth2studio.run.ensemble`.\nIn this case, this is for an ensemble inference workflow that will predict a 5 day\nforecast for Christmas 2022. Following standard Earth2Studio practices, the function\naccepts initialized prognostic, data source, io backend and perturbation method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport time\nfrom datetime import datetime, timedelta\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom earth2studio.utils.coords import map_coords, split_coords\nfrom earth2studio.utils.time import to_time_array\n\ntimes = [datetime(2022, 12, 20)]\nnsteps = 20  # Assuming 6-hour time steps\n\n\ndef christmas_five_day_ensemble(\n    times: list[datetime],\n    nsteps: int,\n    prognostic: PrognosticModel,\n    data: DataSource,\n    io: IOBackend,\n    perturbation: Perturbation,\n    nensemble: int = 8,\n    device: str = \"cuda\",\n) -> None:\n    \"\"\"Ensemble inference example\"\"\"\n    # ==========================================\n    # Fetch Initialization Data\n    prognostic_ic = prognostic.input_coords()\n    times = to_time_array(times)\n\n    x, coords0 = fetch_data(\n        source=data,\n        time=times,\n        variable=prognostic_ic[\"variable\"],\n        lead_time=prognostic_ic[\"lead_time\"],\n        device=device,\n    )\n    # ==========================================\n    # ==========================================\n    # Set up IO backend by pre-allocating arrays (not needed for AsyncZarrBackend)\n    total_coords = prognostic.output_coords(prognostic.input_coords()).copy()\n    if \"batch\" in total_coords:\n        del total_coords[\"batch\"]\n    total_coords[\"time\"] = times\n    total_coords[\"lead_time\"] = np.asarray(\n        [\n            prognostic.output_coords(prognostic.input_coords())[\"lead_time\"] * i\n            for i in range(nsteps + 1)\n        ]\n    ).flatten()\n    total_coords.move_to_end(\"lead_time\", last=False)\n    total_coords.move_to_end(\"time\", last=False)\n    total_coords = {\"ensemble\": np.arange(nensemble)} | total_coords\n\n    variables_to_save = total_coords.pop(\"variable\")\n    io.add_array(total_coords, variables_to_save)\n    # ==========================================\n    # ==========================================\n    # Run inference\n    coords = {\"ensemble\": np.arange(nensemble)} | coords0.copy()\n    x = x.unsqueeze(0).repeat(nensemble, *([1] * x.ndim))\n\n    # Map lat and lon if needed\n    x, coords = map_coords(x, coords, prognostic_ic)\n\n    # Perturb ensemble\n    x, coords = perturbation(x, coords)\n\n    # Create prognostic iterator\n    model = prognostic.create_iterator(x, coords)\n\n    with tqdm(\n        total=nsteps + 1,\n        desc=\"Running batch inference\",\n        position=1,\n        leave=False,\n    ) as pbar:\n        for step, (x, coords) in enumerate(model):\n            # Dump result to IO, split_coords separates variables to different arrays\n            x, coords = map_coords(x, coords, {\"variable\": np.array([\"t2m\", \"tcwv\"])})\n            io.write(*split_coords(x, coords))\n            pbar.update(1)\n            if step == nsteps:\n                break\n    # ==========================================\n\n\ndef get_folder_size(folder_path: str) -> int:\n    \"\"\"Get folder size in megabytes\"\"\"\n    if os.path.isfile(folder_path):\n        return os.path.getsize(folder_path) / (1024 * 1024)\n\n    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(folder_path):\n        for filename in filenames:\n            file_path = os.path.join(dirpath, filename)\n            total_size += os.path.getsize(file_path)\n    return total_size / (1024 * 1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Local Storage Zarr IO\nAs a base line, lets run the Zarr IO backend saving it to local disk.\nLocal IO storage is typically preferred since we can then access the data after the\ninference pipeline is finished using standard libraries.\nChunking play an important role on performance, both with respect to compression and\nalso when accessing data.\nHere we will chunk the output data based on time and lead_time\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "io = ZarrBackend(\n    \"outputs/17_io_sync.zarr\",\n    chunks={\"time\": 1, \"lead_time\": 1},\n    backend_kwargs={\"overwrite\": True},\n)\n\nstart_time = time.time()\nchristmas_five_day_ensemble(times, nsteps, model, ds, io, pt, device=device)\nzarr_local_clock = time.time() - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"\\nLocal zarr store inference time: {zarr_local_clock}s\")\nprint(\n    f\"Uncompressed zarr store size: {get_folder_size('outputs/17_io_sync.zarr'):.2f} MB\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compressed Local Storage Zarr IO\nBy default the Zarr IO backends will be uncompressed.\nIn many instances this is fine, when data volumes are low.\nHowever, in instances that we are writing a very large amount of data or the data\nneeds to get sent over the network to a remote store, compression is essential.\nWith the standard Zarr backend, this will cause a very noticeable slow down, but note\nthat the output store will be 3x smaller!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import zarr\n\nio = ZarrBackend(\n    \"outputs/17_io_sync_compressed.zarr\",\n    chunks={\"time\": 1, \"lead_time\": 1},\n    backend_kwargs={\"overwrite\": True},\n    zarr_codecs=zarr.codecs.BloscCodec(\n        cname=\"zstd\", clevel=3, shuffle=zarr.codecs.BloscShuffle.shuffle\n    ),  # Zarrs default\n)\n\nstart_time = time.time()\nchristmas_five_day_ensemble(times, nsteps, model, ds, io, pt, device=device)\nzarr_local_clock = time.time() - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"\\nLocal compressed zarr store inference time: {zarr_local_clock}s\")\nprint(\n    f\"Compressed zarr store size: {get_folder_size('outputs/17_io_sync_compressed.zarr'):.2f} MB\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Local Storage NetCDF IO\nNetCDF offers a similar user experience but saves the output into a single netCDF\nfile.\nFor local storage, NetCDF it typically preferred since it keeps all outputs into\na single file.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "io = NetCDF4Backend(\"outputs/17_io_sync.nc\", backend_kwargs={\"mode\": \"w\"})\nstart_time = time.time()\nchristmas_five_day_ensemble(times, nsteps, model, ds, io, pt, device=device)\nnc_local_clock = time.time() - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"\\nLocal netcdf store inference time: {nc_local_clock}s\")\nprint(\n    f\"Uncompressed zarr store size: {get_folder_size('outputs/17_io_sync.nc'):.2f} MB\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## In Memory Zarr IO\nOne way we can speed up IO is to save outputs to in-memory stores.\nIn-memory stores more limited in size depending on the hardware being used.\nAlso one needs to be careful with in memory stores, once the Python object is deleted\nthe data is gone.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "io = ZarrBackend(\n    chunks={\"time\": 1, \"lead_time\": 1}, backend_kwargs={\"overwrite\": True}\n)  # Not path = in memory for Zarr\nstart_time = time.time()\nchristmas_five_day_ensemble(times, nsteps, model, ds, io, pt, device=device)\nzarr_memory_clock = time.time() - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"\\nIn memory zarr store inference time: {zarr_memory_clock}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compressed Local Async Zarr IO\nThe async Zarr IO backend is an advanced IO backend designed to offer async\nZarr 3.0 writes to in-memory, local and remote data stores.\nThis data source is ideal when large volumes of data are needed to be written and\nthe users want to mask the IO with the forward execution of the model.\n\nBecause this IO backend relies on both async and multi-threading, it has a different\ninitialization pattern than others.\nThe main difference being that this backend does not use the add_array API, rather\nusers specify `parallel_coords` in the constructor that denote coords that slices will\nbe written to during inference.\nTypically this might be `time`, `lead_time` and `ensemble`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parallel_coords = {\n    \"time\": np.asarray(times),\n    \"lead_time\": np.asarray([timedelta(hours=6 * i) for i in range(nsteps + 1)]),\n}\nio = AsyncZarrBackend(\n    \"outputs/17_io_async.zarr\",\n    parallel_coords=parallel_coords,\n    zarr_codecs=zarr.codecs.BloscCodec(\n        cname=\"zstd\", clevel=3, shuffle=zarr.codecs.BloscShuffle.shuffle\n    ),\n)\nstart_time = time.time()\nchristmas_five_day_ensemble(times, nsteps, model, ds, io, pt, device=device)\nzarr_async_clock = time.time() - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"\\nAsync zarr store inference time: {zarr_async_clock}s\")\nprint(\n    f\"Compressed async zarr store size: {get_folder_size('outputs/17_io_async.zarr'):.2f} MB\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compressed Local Non-Blocking Async Zarr IO\nThat was faster than the normal Zarr method, even the uncompressed version making it\ncomparable to NetCDF, but we can still improve with this IO backend.\nA unique feature of this particular backend is running in non-blocking mode, namely\nIO writes will be placed onto other threads.\nUsers do need to be careful with this to both ensure data is not mutated while the IO\nbackend is working to move the data off the GPU, but also to make sure to wait for\nwrite threads to finish before the object is deleted.\n\nNote that this backend allows Zarr to be comparable to uncompressed NetCDF even 3x\ncompression!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "io = AsyncZarrBackend(\n    \"outputs/17_io_nonblocking_async.zarr\",\n    parallel_coords=parallel_coords,\n    blocking=False,\n    zarr_codecs=zarr.codecs.BloscCodec(\n        cname=\"zstd\", clevel=3, shuffle=zarr.codecs.BloscShuffle.shuffle\n    ),\n)\nstart_time = time.time()\nchristmas_five_day_ensemble(times, nsteps, model, ds, io, pt, device=device)\n# IMPORTANT: Make sure to call close to ensure IO backend threads have finished!\nio.close()\nzarr_nonblocking_async_clock = time.time() - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\n    f\"\\nNon-blocking async zarr store inference time: {zarr_nonblocking_async_clock}s\"\n)\nprint(\n    f\"Compressed non-blocking async zarr store size: {get_folder_size('outputs/17_io_nonblocking_async.zarr'):.2f} MB\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remote Non-Blocking Async Zarr IO\nThis IO backend can be further customized by changing the Fsspec Filesystem used by\nthe Zarr store which can be controlled via the `fs_factory` parameter.\nNote that this is a factory method, the IO backend will need to create multiple\ninstances of the file system.\nSome examples that may be of interest are:\n\n- :code:`from fsspec.implementations.local import LocalFileSystem` (Default, local store)\n- :code:`from fsspec.implementations.memory import MemoryFileSystem` (in-memory store)\n- :code:`from s3fs import S3FileSystem` (Remote S3 store)\n\nFor sake of example, lets have a look at writing to a remote store would require.\nCompression is a must in this instances, since we need to minimize the data transfer\nover the network.\nThe file system factory is set to S3 with the appropiate credentials in a partial\ncallable object.\nLastly we can increase the max number of thread workers with the `pool_size` parameter\nto further boost performance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import functools\n\nimport s3fs\n\nif \"S3FS_KEY\" in os.environ and \"S3FS_SECRET\" in os.environ:\n    # Remember, needs to be a callable\n    fs_factory = functools.partial(\n        s3fs.S3FileSystem,\n        key=os.environ[\"S3FS_KEY\"],\n        secret=os.environ[\"S3FS_SECRET\"],\n        client_kwargs={\"endpoint_url\": os.environ.get(\"S3FS_ENDPOINT\", None)},\n        asynchronous=True,\n    )\n    io = AsyncZarrBackend(\n        \"earth2studio/ci/example/17_io_async.zarr\",\n        parallel_coords=parallel_coords,\n        fs_factory=fs_factory,\n        blocking=False,\n        pool_size=16,\n        zarr_codecs=zarr.codecs.BloscCodec(\n            cname=\"zstd\", clevel=3, shuffle=zarr.codecs.BloscShuffle.shuffle\n        ),\n    )\n    christmas_five_day_ensemble(times, 4, model, ds, io, pt, device=device)\n    # IMPORTANT: Make sure to call close to ensure IO backend threads have finished!\n    io.close()\n\n    # To clean up the zarr store you can use\n    # fs = s3fs.S3FileSystem(\n    #     key=os.environ[\"S3FS_KEY\"],\n    #     secret=os.environ[\"S3FS_SECRET\"],\n    #     client_kwargs={\"endpoint_url\": os.environ.get(\"S3FS_ENDPOINT\", None)},\n    # )\n    # fs.rm(\"earth2studio/ci/example/17_io_async.zarr\", recursive=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-Processing\nLastly, we can plot the each of the local Zarr stores to verify that indeed they are\nthe same.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport xarray as xr\n\n# Load the datasets\nds_async = xr.open_zarr(\"outputs/17_io_async.zarr\", consolidated=False)\nds_nonblocking = xr.open_zarr(\n    \"outputs/17_io_nonblocking_async.zarr\", consolidated=False\n)\nds_sync = xr.open_zarr(\"outputs/17_io_sync.zarr\")\nds_nc = xr.open_dataset(\"outputs/17_io_sync.nc\")\n\n# Create a 2x2 subplot grid\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\nfig.suptitle(\"Comparison of mean t2m across IO Backends\")\n\n# Plot t2m from each dataset\naxes[0, 0].imshow(\n    ds_async.t2m.isel(time=0, lead_time=8).mean(dim=\"ensemble\"), vmin=250, vmax=320\n)\naxes[0, 0].set_title(\"Async Zarr\")\n\naxes[0, 1].imshow(\n    ds_nonblocking.t2m.isel(time=0, lead_time=8).mean(dim=\"ensemble\"),\n    vmin=250,\n    vmax=320,\n)\naxes[0, 1].set_title(\"Non-blocking Async Zarr\")\n\naxes[1, 0].imshow(\n    ds_sync.t2m.isel(time=0, lead_time=8).mean(dim=\"ensemble\"), vmin=250, vmax=320\n)\naxes[1, 0].set_title(\"Sync Zarr\")\n\naxes[1, 1].imshow(\n    ds_nc.t2m.isel(time=0, lead_time=8).mean(dim=\"ensemble\"), vmin=250, vmax=320\n)\naxes[1, 1].set_title(\"NetCDF\")\n\nplt.tight_layout()\nplt.savefig(\"outputs/17_io_performance.jpg\", bbox_inches=\"tight\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}