{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Distributed Manager Inference\n\nSetting up distributed manager for parallel inference\n\nMany inference workflows are embarrassingly parallel and can be easily sharded across\nmultiple devices.\nThis example demonstrates how one can use the Modulus distributed manager to distribute\ninference across mutliple GPUs.\nThe [distributed manager](https://github.com/NVIDIA/modulus/blob/main/modulus/distributed/manager.py)\nis a utility that provides a useful set of properties that pertain to a parallel\nenvironment.\n\nIn this example you will learn:\n\n- How to use the distributed manager to access parallel environment properties\n- Parallelize deterministic inference across multiple initial date-times\n- Limitations of parallel inference in Earth2Studio\n- Post-processing strategies of parallel job outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up\nSet up the distributed manager by initializing it. Out of the box, the distributed\nmanager supports MPI, SLURM and PyTorch parallel environments which provide information\nregarding the parallel enviroment but environment variables.\n\nFor example, this script could be ran using:\n\n```bash\n# OpenMPI\nmpirun -np 2 python 08_distributed_manager.py\n# Torch run\ntorchrun --standalone --nnodes=1 --nproc-per-node=2 08_distributed_manager.py\n```\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>When running in parallel, make sure the .env file in the repository examples\n  folder is *not* present. The .env file is intended for the documentation build only.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nos.makedirs(\"outputs\", exist_ok=True)\nfrom dotenv import load_dotenv\n\nload_dotenv()  # TODO: make common example prep function\n\nimport numpy as np\nimport torch\nfrom loguru import logger\nfrom modulus.distributed import DistributedManager\n\nDistributedManager.initialize()  # Only call this once in the entire script!\ndist = DistributedManager()\nassert (  # noqa: S101\n    dist._distributed\n), \"Looks like torch distributed isn't set up. Check your env variables!\"\n\nlogger.info(\n    f\"Inference runner {dist.rank} of {dist.world_size} with device {dist.device}\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next the needed components get initialized.\nRigorous parallel support is not part of Earth2Studio's design goals, there are some\nspots where potential race conditions can occur. Thus some additional care should be\ntaken to ensure safe parallel inference.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from earth2studio.data import ARCO\nfrom earth2studio.io import ZarrBackend\nfrom earth2studio.models.px import DLWP\n\n# Load model\npackage = DLWP.load_default_package()\nif dist.rank == 0:\n    model = DLWP.load_model(package)\n\ntorch.distributed.barrier()\nif dist.rank != 0:\n    model = DLWP.load_model(package)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When loading models that are built into Earth2Studio, the model's checkpoint files\nwill be downloaded into the machines cache. If each inference process has access to\nthe same cache location, then only one should download the checkpoint triggered by\n:py:func:`load_model`.\n\nHere :py:class:`earth2studio.models.px.DLWP` checkpoint files are first downloaded by\nprocess 0 and then loaded by other processes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create the data source\ndata = ARCO()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The remote date store will place cached data into separate caches for each process.\nThis makes the download of initial state data safe during parallel inference but also\nmeans that multiple jobs will download the same date-time if needed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "chunks = {\"time\": 1}\nio = ZarrBackend(file_name=f\"outputs/08_output_{dist.rank}.zarr\", chunks=chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Earth2Studio does not provide distributed IO support. The recommendation is to always\noutput data for each process to a separate file, then aggregate the data during post\nprocessing.\n\n## Execute the Workflow\nNext we can run the workflow. This example will run inference for a random date across\nseveral years and just save total column water vapor. Shard the initial date-times\nacross the each process. The distributed manager will provide the device ID for the\nprocess.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import earth2studio.run as run\n\ntimes = np.array([f\"200{i:d}-06-01T00:00:00\" for i in range(0, 6)])\nassert (  # noqa: S101\n    len(times) > dist.world_size\n), \"Inference runs should be greater than processes\"\ntime_shard = np.array_split(times, dist.world_size)[dist.rank]\n\nnsteps = 20\noutput_coords = {\"variable\": np.array([\"tcwv\"])}\nio = run.deterministic(\n    time_shard, nsteps, model, data, io, output_coords=output_coords, device=dist.device\n)\n\nprint(io.root.tree())\ntorch.distributed.barrier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post Processing\nFinally, we can post process the results. Xarray provides a useful function for\nopening multiple files as a single dataset, :py:func:`xarray.open_mfdataset`. This\nallows outputs from all processes to get treated as a single data array.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>In this script process 0 is used to post process so the example is in one file.\n  It is best practice to perform post processing in a separate job / script entirely\n  to better utilize compute resources.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if dist.rank == 0:\n    import matplotlib.pyplot as plt\n    import xarray as xr\n\n    from earth2studio.utils.time import timearray_to_datetime\n\n    paths = [f\"outputs/08_output_{i}.zarr\" for i in range(dist.world_size)]\n    ds = xr.open_mfdataset(paths, combine=\"nested\", concat_dim=\"time\", engine=\"zarr\")\n    print(ds)\n\n    ncols = 3\n    fig, ax = plt.subplots(2, ncols, figsize=(12, 6))\n\n    time = timearray_to_datetime(ds.coords[\"time\"].values)\n    for i in range(6):\n        ax[i // ncols, i % ncols].imshow(\n            ds[\"tcwv\"].isel(time=i, lead_time=-1).values,\n            cmap=\"gist_earth\",\n            vmin=0,\n            vmax=100,\n        )\n        ax[i // ncols, i % ncols].set_title(time[i].strftime(\"%m/%d/%Y\"))\n    plt.suptitle(\n        f'TCWV Forecast Lead Time - {ds.coords[\"lead_time\"].values[-1].astype(\"timedelta64[D]\").astype(int)} days'\n    )\n    plt.savefig(\"outputs/08_tcwv_distributed_manager.jpg\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}