{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Statistical Inference\n\nThis example will demonstrate how to run a simple inference workflow to generate a\nforecast and then to save a statistic of that data. There are a handful of built-in\nstatistics available in `earth2studio.statistics`, but here we will demonstrate how\nto define a custom statistic and run inference.\n\nIn this example you will learn:\n\n- How to instantiate a built in prognostic model\n- Creating a data source and IO object\n- Create a custom statistic\n- Running a simple built in workflow\n- Post-processing results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a statistical workflow\n\nStart with creating a simple inference workflow to use. We encourage\nusers to explore and experiment with their own custom workflows that borrow ideas from\nbuilt in workflows inside :py:obj:`earth2studio.run` or the examples.\n\nCreating our own generalizable workflow to use with statistics is easy when we rely on\nthe component interfaces defined in Earth2Studio (use dependency injection). Here we\ncreate a run method that accepts the following:\n\n- time: Input list of datetimes / strings to run inference for\n- nsteps: Number of forecast steps to predict\n- prognostic: Our initialized prognostic model\n- statistic: our custom statistic\n- data: Initialized data source to fetch initial conditions from\n- io: IOBackend\n\nWe do not run an ensemble inference workflow here, even though it is common for statistical\ninference. See ensemble examples for details on how to extend this example for that purpose.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nos.makedirs(\"outputs\", exist_ok=True)\nfrom dotenv import load_dotenv\n\nload_dotenv()  # TODO: make common example prep function\n\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\nfrom loguru import logger\nfrom tqdm import tqdm\n\nfrom earth2studio.data import DataSource, fetch_data\nfrom earth2studio.io import IOBackend\nfrom earth2studio.models.px import PrognosticModel\nfrom earth2studio.statistics import Statistic\nfrom earth2studio.utils.coords import map_coords\nfrom earth2studio.utils.time import to_time_array\n\nlogger.remove()\nlogger.add(lambda msg: tqdm.write(msg, end=\"\"), colorize=True)\n\n\ndef run_stats(\n    time: list[str] | list[datetime] | list[np.datetime64],\n    nsteps: int,\n    nensemble: int,\n    prognostic: PrognosticModel,\n    statistic: Statistic,\n    data: DataSource,\n    io: IOBackend,\n) -> IOBackend:\n    \"\"\"Simple statistics workflow\n\n    Parameters\n    ----------\n    time : list[str] | list[datetime] | list[np.datetime64]\n        List of string, datetimes or np.datetime64\n    nsteps : int\n        Number of forecast steps\n    nensemble : int\n        Number of ensemble members to run inference for.\n    prognostic : PrognosticModel\n        Prognostic models\n    statistic : Statistic\n        Custom statistic to compute and write to IO.\n    data : DataSource\n        Data source\n    io : IOBackend\n        IO object\n\n    Returns\n    -------\n    IOBackend\n        Output IO object\n    \"\"\"\n    logger.info(\"Running simple statistics workflow!\")\n    # Load model onto the device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    logger.info(f\"Inference device: {device}\")\n    prognostic = prognostic.to(device)\n    # Fetch data from data source and load onto device\n    time = to_time_array(time)\n    x, coords = fetch_data(\n        source=data,\n        time=time,\n        lead_time=prognostic.input_coords()[\"lead_time\"],\n        variable=prognostic.input_coords()[\"variable\"],\n        device=device,\n    )\n    logger.success(f\"Fetched data from {data.__class__.__name__}\")\n\n    # Set up IO backend\n    total_coords = coords.copy()\n    output_coords = prognostic.output_coords(prognostic.input_coords())\n    total_coords[\"lead_time\"] = np.asarray(\n        [output_coords[\"lead_time\"] * i for i in range(nsteps + 1)]\n    ).flatten()\n    # Remove reduced dimensions from statistic\n    for d in statistic.reduction_dimensions:\n        total_coords.pop(d, None)\n\n    io.add_array(total_coords, str(statistic))\n\n    # Map lat and lon if needed\n    x, coords = map_coords(x, coords, prognostic.input_coords())\n\n    # Create prognostic iterator\n    model = prognostic.create_iterator(x, coords)\n\n    logger.info(\"Inference starting!\")\n    with tqdm(total=nsteps + 1, desc=\"Running inference\") as pbar:\n        for step, (x, coords) in enumerate(model):\n            s, coords = statistic(x, coords)\n            io.write(s, coords, str(statistic))\n            pbar.update(1)\n            if step == nsteps:\n                break\n\n    logger.success(\"Inference complete\")\n    return io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up\nWith the statistical workflow defined, we now need to create the individual components.\n\nWe need the following:\n\n- Prognostic Model: Use the built in Pangu 24 hour model :py:class:`earth2studio.models.px.Pangu24`.\n- statistic: We define our own statistic: the Southern Oscillation Index (SOI).\n- Datasource: Pull data from the GFS data api :py:class:`earth2studio.data.GFS`.\n- IO Backend: Save the outputs into a NetCDF4 store :py:class:`earth2studio.io.NetCDF4Backend`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n\nimport numpy as np\nimport torch\n\nfrom earth2studio.data import GFS\nfrom earth2studio.io import NetCDF4Backend\nfrom earth2studio.models.px import Pangu24\nfrom earth2studio.utils.type import CoordSystem\n\n# Load the default model package which downloads the check point from NGC\npackage = Pangu24.load_default_package()\nmodel = Pangu24.load_model(package)\n\n# Create the data source\ndata = GFS()\n\n# Create the IO handler, store in memory\nio = NetCDF4Backend(\n    file_name=\"outputs/soi.nc\",\n    backend_kwargs={\"mode\": \"w\"},\n)\n\n\n# Create the custom statistic\nclass SOI:\n    \"\"\"Custom metric calculation the Southern Oscillation Index.\n\n    SOI = ( standardized_tahiti_slp - standardized_darwin_slp ) / soi_normalization\n\n    soi_normalization = std( historical ( standardized_tahiti_slp - standardized_darwin_slp ) )\n\n    standardized_*_slp = (*_slp - climatological_mean_*_slp) / climatological_std_*_slp\n\n    Note\n    ----\n    __str__\n        Name that will be applied to the output of this statistic, primarily for IO purposes.\n    reduction_dimensions\n        Dimensions that this statistic reduces over. This is used to help automatically determine\n        the output coordinates, primarily used for IO purposes.\n    \"\"\"\n\n    def __str__(self) -> str:\n        return \"soi\"\n\n    def __init__(\n        self,\n    ):\n        # Read in Tahiti and Darwin SLP data\n        from physicsnemo.utils.filesystem import _download_cached\n\n        file_path = _download_cached(\n            \"http://data.longpaddock.qld.gov.au/SeasonalClimateOutlook/SouthernOscillationIndex/SOIDataFiles/DailySOI1933-1992Base.txt\"\n        )\n        ds = pd.read_csv(file_path, sep=r\"\\s+\")\n        dates = pd.date_range(\"1999-01-01\", freq=\"d\", periods=len(ds))\n        ds[\"date\"] = dates\n        ds = ds.set_index(\"date\")\n        ds = ds.drop([\"Year\", \"Day\", \"SOI\"], axis=1)\n        ds = ds.rolling(30, min_periods=1).mean().dropna()\n\n        self.climatological_means = torch.tensor(\n            ds.groupby(ds.index.month).mean().to_numpy(), dtype=torch.float32\n        )\n        self.climatological_std = torch.tensor(\n            ds.groupby(ds.index.month).std().to_numpy(), dtype=torch.float32\n        )\n\n        standardized = ds.groupby(ds.index.month).transform(\n            lambda x: (x - x.mean()) / x.std()\n        )\n        diff = standardized[\"Tahiti\"] - standardized[\"Darwin\"]\n\n        self.normalization = torch.tensor(\n            diff.groupby(ds.index.month).std().to_numpy(), dtype=torch.float32\n        )\n\n        self.tahiti_coords = {\n            \"variable\": np.array([\"msl\"]),\n            \"lat\": np.array([-17.65]),\n            \"lon\": np.array([210.57]),\n        }\n        self.darwin_coords = {\n            \"variable\": np.array([\"msl\"]),\n            \"lat\": np.array([-12.46]),\n            \"lon\": np.array([130.84]),\n        }\n\n        self.reduction_dimensions = list(self.tahiti_coords)\n\n    def __call__(\n        self, x: torch.Tensor, coords: CoordSystem\n    ) -> tuple[torch.Tensor, CoordSystem]:\n        \"\"\"Computes the SOI given an input.\n\n        coords must be a superset of both\n\n        tahiti_coords = {\n            'variable': np.array(['msl']),\n            'lat': np.array([-17.65]),\n            'lon': np.array([210.57])\n        }\n\n        and\n\n        darwin_coords = {\n            'variable': np.array(['msl']),\n            'lat': np.array([-12.46]),\n            'lon': np.array([130.84])\n        }\n\n        So make sure that the model chosen predicts the `msl` variable.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor\n        coords : CoordSystem\n            coordinate system belonging to the input tensor.\n\n        Returns\n        -------\n        tuple[torch.Tensor, CoordSystem]\n            Returns the SOI and appropriate coordinate system.\n        \"\"\"\n        tahiti, _ = map_coords(x, coords, self.tahiti_coords)\n        darwin, _ = map_coords(x, coords, self.darwin_coords)\n\n        tahiti = tahiti.squeeze(-3, -2, -1) / 100.0\n        darwin = darwin.squeeze(-3, -2, -1) / 100.0\n        output_coords = OrderedDict(\n            {k: v for k, v in coords.items() if k not in self.reduction_dimensions}\n        )\n\n        # Get time coordinates\n        times = coords[\"time\"].reshape(-1, 1) + coords[\"lead_time\"].reshape(1, -1)\n        months = torch.broadcast_to(\n            torch.as_tensor(\n                [pd.Timestamp(t).month for t in times.flatten()],\n                device=tahiti.device,\n                dtype=torch.int32,\n            ).reshape(times.shape),\n            tahiti.shape,\n        )\n\n        cm = self.climatological_means.to(tahiti.device)\n        cs = self.climatological_std.to(tahiti.device)\n        norm = self.normalization.to(tahiti.device)\n\n        tahiti_std_anomaly = (tahiti - cm[months, 0]) / cs[months, 0]\n        darwin_std_anomaly = (tahiti - cm[months, 1]) / cs[months, 1]\n\n        return (tahiti_std_anomaly - darwin_std_anomaly) / norm[months], output_coords\n\n\nsoi = SOI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute the Workflow\nWith all components initialized, running the workflow is a single line of Python code.\nWorkflow will return the provided IO object back to the user, which can be used to\nthen post process. Some have additional APIs that can be handy for post-processing or\nsaving to file. Check the API docs for more information.\nWe simulate a trajectory of 60 time steps, or 2 months using Pangu24\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nsteps = 60\nnensemble = 1\nio = run_stats([\"2022-01-01\"], nsteps, nensemble, model, soi, data, io)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post Processing\nThe last step is to post process our results.\n\nNotice that the NetCDF IO function has additional APIs to interact with the stored data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\ntimes = io[\"time\"][:].flatten() + io[\"lead_time\"][:].flatten()\n\nfig = plt.figure(figsize=(12, 4))\nax = fig.add_subplot(1, 1, 1)\nax.plot(times, io[\"soi\"][:].flatten())\nax.set_title(\"Southern Oscillation Index\")\nax.grid(\"on\")\n\nplt.savefig(\"outputs/07_southern_oscillation_index_prediction_2022.png\")\nio.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}