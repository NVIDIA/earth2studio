{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Creating a Local Data Source\n\nCreate and save an offline dataset to use in an inference pipeline.\n\nThis example demonstrates how to:\n\n- Build a small offline dataset by fetching data and writing to a Zarr store\n- Load the local store as a data source for an inference pipeline with the Microsoft Aurora model\n- Run the deterministic workflow and plot results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# /// script\n# dependencies = [\n#   \"earth2studio @ git+https://github.com/NVIDIA/earth2studio.git\",\n#   \"aurora\",\n#   \"cartopy\",\n# ]\n# ///"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up\nFor this example, the following are needed:\n\n- Prognostic Model: Use the built-in Aurora 6-hour model :py:class:`earth2studio.models.px.Aurora`.\n- Data source: Pull data from the WeatherBench2 data API :py:class:`earth2studio.data.WB2ERA5`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nos.makedirs(\"outputs\", exist_ok=True)\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nfrom earth2studio.data import WB2ERA5, fetch_data\nfrom earth2studio.models.px import Aurora\n\n# Load the default model package which downloads the checkpoint from GCP\npackage = Aurora.load_default_package()\nmodel = Aurora.load_model(package)\n\n# Create the data source, cache is false\nwb2 = WB2ERA5(cache=False, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Local Zarr Store from a Datasource\nStart with creating a local dataset from the WeatherBench2 data store. Since data\nsources return in-memory data arrays, there are a variety of ways this could be done.\nThe following is a simple method using Earth2Studio IO objects to pack the requested\ndata into a single Zarr store.\n\nFor this example, let's download some data for a Microsoft aurora forecast.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n\nimport numpy as np\n\nfrom earth2studio.io import ZarrBackend\nfrom earth2studio.utils.coords import split_coords\n\ntimes = np.array(\n    [np.datetime64(\"2022-01-01T00:00:00\"), np.datetime64(\"2022-01-01T06:00:00\")]\n)\nvariables = model.input_coords()[\"variable\"]\nzarr_path = \"./outputs/19_wb2_dataset.zarr\"\n# Create Zarr store to pack data into\nzb = ZarrBackend(file_name=zarr_path, backend_kwargs={\"overwrite\": True})\nfull_coords = OrderedDict(\n    [\n        (\"time\", np.atleast_1d(times)),\n        (\"lead_time\", np.array([np.timedelta64(0, \"h\")])),\n        (\"lat\", np.linspace(90, -90, 721)),\n        (\"lon\", np.linspace(0, 359.75, 1440)),\n    ]\n)\nzb.add_array(full_coords, array_name=list(variables))\n\n# Loop over timestamps, fetch data and write slices into the pre-created arrays\nfor t in np.atleast_1d(times):\n    x, coords = fetch_data(\n        wb2,\n        time=np.array([t]),\n        variable=variables,\n        lead_time=np.array([np.timedelta64(0, \"h\")]),\n        device=\"cpu\",\n    )\n    xs, reduced_coords, var_names = split_coords(x, coords, dim=\"variable\")\n    zb.write(xs, reduced_coords, array_name=list(var_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the Zarr store we just created can be used for more than just Earth2Studio\ninference pipelines. Open it with zarr or xarray to explore/process what\nyou just downloaded.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import zarr\n\nzg = zarr.group(store=zarr.storage.LocalStore(zarr_path))\nprint(zg.tree())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute the Workflow\nTo use the saved dataset as a data source, we could create our own class that implements\nthe interface required by :py:class:`earth2studio.data.base.DataSource`, which needs\njust a ``__call__(time, variable)`` method.\n\nHowever, since we used an IO backend from Earth2Studio we can use the\n:py:class:`earth2studio.data.xr.InferenceOutputSource` which is a convenience class\nthat supports the output of inference pipelines.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import earth2studio.run as run\nfrom earth2studio.data import InferenceOutputSource\n\noffline_source = InferenceOutputSource(zarr_path)\nout_zarr_path = \"./outputs/19_pangu_output.zarr\"\nio = ZarrBackend(file_name=out_zarr_path, backend_kwargs={\"overwrite\": True})\nio = run.deterministic(\n    times[-1:],\n    4,\n    model,\n    offline_source,\n    io,\n    output_coords=OrderedDict({\"variable\": np.array([\"msl\"])}),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post Processing\nThe last step is to post-process our results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\n\nplt.close(\"all\")\nprojection = ccrs.Robinson()\nfig, axes = plt.subplots(\n    2,\n    2,\n    subplot_kw={\"projection\": projection},\n    figsize=(12, 7),\n    constrained_layout=True,\n)\naxes = axes.ravel()\n\nlon = io[\"lon\"][:]\nlat = io[\"lat\"][:]\nlead_steps = [1, 2, 3, 4]  # 6h, 12h, 18h, 24h\nfor ax, step in zip(axes, lead_steps):\n    im = ax.pcolormesh(\n        lon,\n        lat,\n        io[\"msl\"][0, step],\n        transform=ccrs.PlateCarree(),\n        cmap=\"PiYG\",\n    )\n    ax.set_title(f\"msl - Lead time: {6*step}h\")\n    ax.coastlines()\n    ax.gridlines(draw_labels=False)\n\nfig.colorbar(\n    im, ax=axes, orientation=\"horizontal\", fraction=0.05, pad=0.07, label=\"msl\"\n)\nplt.savefig(\"outputs/19_msl_1day.png\", dpi=150)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}