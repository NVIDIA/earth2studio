{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Huge Ensembles (HENS) Checkpoints\n\nBasic multi-checkpoint Huge Ensembles (HENS) inference workflow.\n\nThis example provides a basic example to load the Huge Ensemble checkpoints to perform\nensemble inference.\nThis notebook aims to demonstrate the foundations of running a multi-checkpoint workflow\nfrom Earth2Studio components.\nFor more details about HENS, see:\n\n- https://arxiv.org/abs/2408.03100\n- https://github.com/ankurmahesh/earth2mip-fork\n\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>We encourage users to familiarize themselves with the license restrictions of this\n    model's checkpoints.</p></div>\n\nFor the complete HENS workflow, we encourage users to have a look at the HENS recipe\nwhich provides a end-to-end solution to leverage HENS for downstream analysis such as\ntropical cyclone tracking:\n\n- https://github.com/NVIDIA/earth2studio/tree/main/recipes/hens\n\nIn this example you will learn:\n\n- How to load the HENS checkpoints with a custom model package\n- How to load the HENS perturbation method\n- How to create a simple ensemble inference loop\n- How to visualize results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# /// script\n# dependencies = [\n#   \"earth2studio[sfno] @ git+https://github.com/NVIDIA/earth2studio.git@0.12.0\",\n#   \"cartopy\",\n# ]\n# ///"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up\nFirst, import the necessary modules and set up our environment and load the required\nmodules.\nHENS has checkpoints conveniently stored on [HuggingFace](https://huggingface.co/datasets/maheshankur10/hens/tree/main/earth2mip_prod_registry)\nthat we will use.\nRather than loading the default checkpoint from the original SFNO paper, create a\nmodel package that points to the specific HENS checkpoint we want to use instead.\n\nThis example also needs the following:\n\n- Prognostic Base Model: Use SFNO model architecture :py:class:`earth2studio.models.px.SFNO`.\n- Datasource: Pull data from the GFS data api :py:class:`earth2studio.data.GFS`.\n- Perturbation Method: HENS uses a novel perturbation method :py:class:`earth2studio.perturbation.HemisphericCentredBredVector`.\n- Seeding Perturbation Method: Perturbation method to seed the Bred Vector :py:class:`earth2studio.perturbation.CorrelatedSphericalGaussian`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nos.makedirs(\"outputs\", exist_ok=True)\nfrom dotenv import load_dotenv\n\nload_dotenv()  # TODO: make common example prep function\n\nfrom earth2studio.data import GFS\nfrom earth2studio.io import ZarrBackend\nfrom earth2studio.models.auto import Package\nfrom earth2studio.models.px import SFNO\nfrom earth2studio.perturbation import (\n    CorrelatedSphericalGaussian,\n    HemisphericCentredBredVector,\n)\nfrom earth2studio.run import ensemble\n\n# Set up two model packages for each checkpoint\n# Note the modification of the cache location to avoid overwriting\nmodel_package_1 = Package(\n    \"hf://datasets/maheshankur10/hens/earth2mip_prod_registry/sfno_linear_74chq_sc2_layers8_edim620_wstgl2-epoch70_seed102\",\n    cache_options={\n        \"cache_storage\": Package.default_cache(\"hens_1\"),\n        \"same_names\": True,\n    },\n)\n\nmodel_package_2 = Package(\n    \"hf://datasets/maheshankur10/hens/earth2mip_prod_registry/sfno_linear_74chq_sc2_layers8_edim620_wstgl2-epoch70_seed103\",\n    cache_options={\n        \"cache_storage\": Package.default_cache(\"hens_2\"),\n        \"same_names\": True,\n    },\n)\n\n# Create the data source\ndata = GFS()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute the Workflow\nNext we execute the ensemble workflow for each model but loop through each checkpoint.\nNote that the models themselves have not been loaded into memory yet, this will be\ndone one at a time to minimize the memory footprint of inference on a GPU.\nBefore the ensemble workflow can get executed the following set up is needed:\n\n- Initialize the SFNO model from checkpoint\n- Initialize the perturbation method with the prognostic model\n- Initialize the IO zarr store for this model\n\nIf multiple GPUs are being used, one could parallelize inference using different\ncheckpoints on each card.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import gc\nfrom datetime import datetime, timedelta\n\nimport numpy as np\nimport torch\n\nstart_date = datetime(2024, 1, 1)\nnsteps = 4\nnensemble = 2\n\nfor i, package in enumerate([model_package_1, model_package_2]):\n    # Load SFNO model from package\n    # HENS checkpoints use different inputs than default SFNO (inclusion of d2m)\n    # Can find this in the config.json, the load_model function in SFNO handles this\n    model = SFNO.load_model(package)\n\n    # Perturbation method\n    # Here we will simplify the process that's in the original paper for conciseness\n    noise_amplification = torch.zeros(model.input_coords()[\"variable\"].shape[0])\n    index_z500 = list(model.input_coords()[\"variable\"]).index(\"z500\")\n    noise_amplification[index_z500] = 39.27  # z500 (0.35 * z500 skill)\n    noise_amplification = noise_amplification.reshape(1, 1, 1, -1, 1, 1)\n\n    seed_perturbation = CorrelatedSphericalGaussian(noise_amplitude=noise_amplification)\n    perturbation = HemisphericCentredBredVector(\n        model, data, seed_perturbation, noise_amplitude=noise_amplification\n    )\n\n    # IO object\n    io = ZarrBackend(\n        file_name=f\"outputs/11_hens_{i}.zarr\",\n        chunks={\"ensemble\": 1, \"time\": 1, \"lead_time\": 1},\n        backend_kwargs={\"overwrite\": True},\n    )\n\n    io = ensemble(\n        [\"2024-01-01\"],\n        nsteps,\n        nensemble,\n        model,\n        data,\n        io,\n        perturbation,\n        batch_size=1,\n        output_coords={\"variable\": np.array([\"u10m\", \"v10m\"])},\n    )\n\n    print(io.root.tree())\n    # Do some manual clean up to free up VRAM\n    del model\n    del perturbation\n    gc.collect()\n    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post Processing\nThe result of the workflow is two zarr stores with the ensemble data for the\nrespective checkpoints used.\nThe rest of the example is focused on some basic post processing to visualize the\nresults.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport xarray as xr\n\nlead_time = 4\nplot_date = start_date + timedelta(hours=int(6 * lead_time))\n\n# Load data from both zarr stores\nds0 = xr.open_zarr(\"outputs/11_hens_0.zarr\")\nds1 = xr.open_zarr(\"outputs/11_hens_1.zarr\")\n\n# Combine the datasets\nds = xr.concat([ds0, ds1], dim=\"ensemble\")\n\n# Calculate wind speed magnitude\nwind_speed = np.sqrt(ds.u10m**2 + ds.v10m**2)\n\n# Get mean and std of 4th timestep across ensemble\nmean_wind = wind_speed.isel(time=0, lead_time=lead_time).mean(dim=\"ensemble\")\nstd_wind = wind_speed.isel(time=0, lead_time=lead_time).std(dim=\"ensemble\")\n\n# Create figure with two subplots\nfig, (ax1, ax2) = plt.subplots(\n    1, 2, figsize=(15, 4), subplot_kw={\"projection\": ccrs.PlateCarree()}\n)\n\n# Plot mean\np1 = ax1.contourf(\n    mean_wind.coords[\"lon\"],\n    mean_wind.coords[\"lat\"],\n    mean_wind,\n    levels=15,\n    transform=ccrs.PlateCarree(),\n    cmap=\"nipy_spectral\",\n)\nax1.coastlines()\nax1.set_title(f'Mean Wind Speed\\n{plot_date.strftime(\"%Y-%m-%d %H:%M UTC\")}')\nfig.colorbar(p1, ax=ax1, label=\"m/s\")\n\n# Plot standard deviation\np2 = ax2.contourf(\n    std_wind.coords[\"lon\"],\n    std_wind.coords[\"lat\"],\n    std_wind,\n    levels=15,\n    transform=ccrs.PlateCarree(),\n    cmap=\"viridis\",\n)\nax2.coastlines()\nax2.set_title(\n    f'Wind Speed Standard Deviation\\n{plot_date.strftime(\"%Y-%m-%d %H:%M UTC\")}'\n)\nfig.colorbar(p2, ax=ax2, label=\"m/s\")\n\nplt.tight_layout()\n# Save the figure\nplt.savefig(f\"outputs/11_hens_step_{plot_date.strftime('%Y_%m_%d')}.jpg\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}