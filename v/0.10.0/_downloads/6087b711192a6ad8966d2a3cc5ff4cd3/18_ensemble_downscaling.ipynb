{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Ensemble Forecasting with Downscaling\n\nCustom ensembling workflow with generative downscaling using CorrDiff.\n\n\nThis example demonstrates an ensemble forecasting pipeline that runs a\nprognostic model ensemble and applies CorrDiff generative downscaling to each\nstep and member. While this example uses SFNO for the prognostic model, the\npipeline is model-agnostic and can be reused with other prognostic models that\nfollow the Earth2Studio interfaces.\n\nIn this example you will learn:\n\n- How to create an ensemble forecast pipeline with CorrDiff downscaling\n- Saving output ensemble data to a Zarr store\n- Post-process results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# /// script\n# dependencies = [\n#     \"earth2studio[sfno] @ git+https://github.com/NVIDIA/earth2studio.git\",\n#     \"earth2studio[corrdiff] @ git+https://github.com/NVIDIA/earth2studio.git\",\n#     \"cartopy\",\n#     \"matplotlib\",\n# ]\n# ///"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating an Ensemble Downscaling Workflow\n\nTo create our own ensemble forecasting with downscaling workflow, we will use the\nbuilt-in ensemble workflow :py:meth:`earth2studio.run.ensemble` as the reference to\nstart with. For this to work we need to update how the output coordinates are\ncalculated for the IO object, as well as add the CorrDiff model's forward call\ninto the forecast loop.\n\nAs in previous examples, we use dependency injection to define the signature of the\npipeline method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nos.makedirs(\"outputs\", exist_ok=True)\nfrom dotenv import load_dotenv\n\nload_dotenv()  # TODO: make common example prep function\n\nfrom datetime import datetime\nfrom math import ceil\n\nimport numpy as np\nimport torch\nfrom loguru import logger\nfrom tqdm import tqdm\n\nfrom earth2studio.data import DataSource, fetch_data\nfrom earth2studio.io import IOBackend\nfrom earth2studio.models.dx import CorrDiff\nfrom earth2studio.models.px import PrognosticModel\nfrom earth2studio.perturbation import Perturbation\nfrom earth2studio.utils.coords import map_coords, split_coords\nfrom earth2studio.utils.time import to_time_array\n\n\ndef corrdiff_on_hens_ensemble(\n    time: list[str] | list[datetime] | list[np.datetime64],\n    nsteps: int,\n    nensemble: int,\n    nsamples: int,\n    prognostic: PrognosticModel,\n    corrdiff: CorrDiff,\n    data: DataSource,\n    io: IOBackend,\n    perturbation: Perturbation,\n    batch_size: int | None = None,\n) -> IOBackend:\n    \"\"\"Ensemble CorrDiff pipeline\n\n    Parameters\n    ----------\n    time : list[str] | list[datetime] | list[np.datetime64]\n        Forecast start times\n    nsteps : int\n        Number of forecast steps for prognostic model to take\n    nensemble : int\n        Number of forecast ensemble members\n    nsamples : int\n        Number of samples from CorrDiff model to generate\n    prognostic : PrognosticModel\n        Prognostic model\n    corrdiff : CorrDiff\n        CorrDiff model\n    data : DataSource\n        Data source\n    io : IOBackend\n        IO Backend\n    perturbation : Perturbation\n        Perturbation method\n    batch_size : int | None, optional\n        Ensemble batch size during forecasting. If None, uses nensemble; default None\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    prognostic = prognostic.to(device)\n    corrdiff = corrdiff.to(device)\n    corrdiff.number_of_samples = nsamples\n\n    # Fetch initial data for the ensemble\n    prognostic_ic = prognostic.input_coords()\n    time = to_time_array(time)\n    x0, coords0 = fetch_data(\n        source=data,\n        time=time,\n        variable=prognostic_ic[\"variable\"],\n        lead_time=prognostic_ic[\"lead_time\"],\n        device=device,\n        interp_to=prognostic_ic if hasattr(prognostic, \"interp_method\") else None,\n        interp_method=getattr(prognostic, \"interp_method\", \"nearest\"),\n    )\n\n    # Prepare CorrDiff output coordinates for IO backend\n    total_coords = corrdiff.output_coords(corrdiff.input_coords())\n    if \"batch\" in total_coords:\n        del total_coords[\"batch\"]\n    total_coords[\"time\"] = time\n    total_coords[\"lead_time\"] = np.asarray(\n        [\n            prognostic.output_coords(prognostic.input_coords())[\"lead_time\"] * i\n            for i in range(nsteps + 1)\n        ]\n    ).flatten()\n    total_coords[\"ensemble\"] = np.arange(nensemble)\n    total_coords.move_to_end(\"lead_time\", last=False)\n    total_coords.move_to_end(\"time\", last=False)\n    total_coords.move_to_end(\"ensemble\", last=False)\n    variables_to_save = total_coords.pop(\"variable\")\n    io.add_array(total_coords, variables_to_save)\n\n    # Determine batch size and number of batches\n    batch_size = min(nensemble, batch_size or nensemble)\n    number_of_batches = ceil(nensemble / batch_size)\n    logger.info(\n        f\"Starting {nensemble} member ensemble inference with {number_of_batches} batches.\"\n    )\n\n    # Main ensemble loop\n    for batch_id in tqdm(\n        range(0, nensemble, batch_size),\n        total=number_of_batches,\n        desc=\"Ensemble Batches\",\n    ):\n        mini_batch_size = min(batch_size, nensemble - batch_id)\n        x = x0.to(device)\n        # Set up coordinates for this batch\n        coords = {\n            \"ensemble\": np.arange(batch_id, batch_id + mini_batch_size),\n            **coords0.copy(),\n        }\n        # Repeat initial condition for each ensemble member in the batch\n        x = x.unsqueeze(0).repeat(mini_batch_size, *([1] * x.ndim))\n        x, coords = map_coords(x, coords, prognostic_ic)\n        x, coords = perturbation(x, coords)\n        model = prognostic.create_iterator(x, coords)\n\n        with tqdm(\n            total=nsteps + 1,\n            desc=f\"Batch {batch_id} inference\",\n            position=1,\n            leave=False,\n        ) as pbar:\n            for step, (x, coords) in enumerate(model):\n                # Map prognostic outputs to CorrDiff inputs if needed\n                x, coords = map_coords(x, coords, corrdiff.input_coords())\n                # CorrDiff workflow: generate and write CorrDiff outputs\n                x, coords = corrdiff(x, coords)\n                io.write(*split_coords(x, coords))\n                pbar.update(1)\n                if step == nsteps:\n                    break\n\n    logger.success(\"Inference complete\")\n    return io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up\nWith the inference pipeline function defined, next let's create the required\ncomponents as usual. We need the following:\n\n- Prognostic Model: Use the built in SFNO model :py:class:`earth2studio.models.px.SFNO`.\n- CorrDiff Model: Use the built in CorrDiff Taiwan model :py:class:`earth2studio.models.dx.CorrDiffTaiwan`.\n- Datasource: Pull data from the GFS data api :py:class:`earth2studio.data.GFS`.\n- IO Backend: Let's save the outputs into a Zarr store :py:class:`earth2studio.io.ZarrBackend`.\n\nFor the prognostic checkpoint, we will use a HENS checkpoint conveniently stored\non [HuggingFace](https://huggingface.co/datasets/maheshankur10/hens/tree/main/earth2mip_prod_registry).\nRefer to the previous examples for more information about loading these models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from earth2studio.data import GFS\nfrom earth2studio.io import ZarrBackend\nfrom earth2studio.models.auto import Package\nfrom earth2studio.models.dx import CorrDiffTaiwan\nfrom earth2studio.models.px import SFNO\nfrom earth2studio.perturbation import (\n    CorrelatedSphericalGaussian,\n    HemisphericCentredBredVector,\n)\n\n# Create data source\ndata = GFS()\n# Load prognostic model\nhens_package = Package(\n    \"hf://datasets/maheshankur10/hens/earth2mip_prod_registry/sfno_linear_74chq_sc2_layers8_edim620_wstgl2-epoch70_seed103\",\n    cache_options={\n        \"cache_storage\": Package.default_cache(\"hens_1\"),\n        \"same_names\": True,\n    },\n)\nmodel = SFNO.load_model(hens_package)\n# Set up perturbation method\nnoise_amplification = torch.zeros(model.input_coords()[\"variable\"].shape[0])\nindex_z500 = list(model.input_coords()[\"variable\"]).index(\"z500\")\nnoise_amplification[index_z500] = 39.27\nnoise_amplification = noise_amplification.reshape(1, 1, 1, -1, 1, 1)\nseed_perturbation = CorrelatedSphericalGaussian(noise_amplitude=noise_amplification)\nperturbation = HemisphericCentredBredVector(\n    model, data, seed_perturbation, noise_amplitude=noise_amplification\n)\n# Load the CorrDiffTaiwan model\ncorrdiff = CorrDiffTaiwan.load_model(CorrDiffTaiwan.load_default_package())\n# Set up IO backend\nio = ZarrBackend(\n    file_name=\"outputs/18_ensemble_corrdiff.zarr\",\n    chunks={\"ensemble\": 1, \"sample\": 1, \"time\": 1, \"lead_time\": 1},\n    backend_kwargs={\"overwrite\": True},\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run\nExecute the pipeline. For this example, we use the period of Typhoon Doksuri which\nhas a track over the Taiwan region.\nhttps://en.wikipedia.org/wiki/Typhoon_Doksuri\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "start_date = datetime(2023, 7, 26, 12)\ncorrdiff_on_hens_ensemble(\n    time=[start_date],\n    nsteps=4,\n    nensemble=2,\n    nsamples=3,\n    prognostic=model,\n    corrdiff=corrdiff,\n    data=data,\n    io=io,\n    perturbation=perturbation,\n    batch_size=1,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-processing\nPlot the mean and standard deviation of 10m wind speed magnitude for a sequence of\nlead times.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\nimport xarray as xr\n\nds = xr.open_zarr(\"outputs/18_ensemble_corrdiff.zarr\")\nlead_time = 4\narr = np.sqrt(ds[\"u10m\"] ** 2 + ds[\"v10m\"] ** 2)\nmean_field = arr.mean(dim=[\"ensemble\", \"sample\"])\nstd_field = arr.std(dim=[\"ensemble\", \"sample\"])\nfig, ax = plt.subplots(\n    2, lead_time, figsize=(12, 5), subplot_kw={\"projection\": ccrs.PlateCarree()}\n)\n\nfor i in range(lead_time):\n    p1 = ax[0, i].contourf(\n        ds[\"lon\"],\n        ds[\"lat\"],\n        mean_field.isel(time=0, lead_time=i),\n        levels=20,\n        vmin=0,\n        vmax=40,\n        transform=ccrs.PlateCarree(),\n        cmap=\"nipy_spectral\",\n    )\n    ax[0, i].coastlines()\n    ax[0, i].set_title(f\"lead_time={6*i}hr\")\n\n    p2 = ax[1, i].contourf(\n        ds[\"lon\"],\n        ds[\"lat\"],\n        std_field.isel(time=0, lead_time=i),\n        levels=20,\n        vmin=0,\n        vmax=4,\n        transform=ccrs.PlateCarree(),\n        cmap=\"magma\",\n    )\n    ax[1, i].coastlines()\n\nfig.colorbar(p1, ax=ax[0, -1], label=\"wind speed mean\")\nfig.colorbar(p2, ax=ax[1, -1], label=\"wind speed std\")\nfig.suptitle(\n    f\"Start date: {np.datetime_as_string(ds['time'].values[0], unit='h')}\", fontsize=12\n)\n\n# Leave room for suptitle\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.savefig(\"outputs/18_ensemble_corrdiff_w10m.jpg\")\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}