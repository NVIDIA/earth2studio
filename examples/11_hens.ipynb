{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HENS\n",
    "\n",
    "## Background\n",
    "\n",
    "HENS (huge ensembles), as described in [Huge Ensembles Part I](https://arxiv.org/abs/2408.03100) provides an AI inference system that produces calibrated ensembles. \n",
    "The `11_hens` directory contains a scalable ensembling system pipeline that implements HENS. The pipeline has been designed with flexibility in mind, allowing it to be used with various models and perturbation methods.\n",
    "\n",
    "This notebook provides an overview of the key concepts in the pipeline by demonstrating its application to a small ensemble of Hurricane Helene. For more detailed information and examples of running HENS at scale, please refer to the comprehensive README in the `11_hens/` directory and the various configuration examples therein.\n",
    "While this notebook can be adapted for larger ensembles, we recommend using the `11_hens/` folder for such cases.\n",
    "\n",
    "## This example here\n",
    "\n",
    "In this notebook, we will examine ensemble generation for Hurricane Helene, a tropical cyclone that made landfall in September 2024. The storm posed a challenging case for weather prediction systems and caused widespread impacts across the southeastern United States.\n",
    "\n",
    "The workflow is structured as follows: first, we will set up the most important configurations and initialise key objects, then explore their content. Following this, we will assemble the perturbation before running the inference. Finally, we will visualise the results by plotting the tracks and fields.\n",
    "\n",
    "**NOTE**: provide links to download (at least two) checkpoints and skill file.\n",
    "\n",
    "## Configuring the pipeline\n",
    "\n",
    "The pipeline requires several configuration parameters to be set. We will define the most important ones here and then combine them into a configuration object. The key parameters include:\n",
    "- `project`: project name used for output file naming\n",
    "- `start_times`: time of initial conditions (multiple ICs can be specified)\n",
    "- `nsteps`: number of forecast steps\n",
    "- `nensemble`: ensemble size **per checkpoint and IC**\n",
    "- `batch_size`: number of forecast steps to run in parallel\n",
    "- `model_packages`: path to the model packages\n",
    "- `max_num_checkpoints`: maximum number of checkpoints to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'helene'\n",
    "\n",
    "start_times = [\"2024-09-24 12:00:00\"]\n",
    "nsteps = 16\n",
    "nensemble = 4\n",
    "batch_size = 2\n",
    "\n",
    "model_packages = '/media/mkoch/9ee63bf8-5a14-4872-86f2-7f16b120269b/hens_data/hens_checkpoints'\n",
    "max_num_checkpoints = 2\n",
    "\n",
    "output_vars = [\"t2m\", 'u10m', 'v10m', 'u850', 'v850', 'msl', 'z500']\n",
    "out_dir = './outputs'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us imports some required features and fully configure the inference using the parameters set above. For more details on the configurations, have a look at the README in the `11_hens/` folder and explore the configs therein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig\n",
    "from physicsnemo.distributed import DistributedManager\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '11_hens'))\n",
    "load_dotenv()\n",
    "DistributedManager.initialize()\n",
    "\n",
    "# Create the configuration dictionary\n",
    "cfg = DictConfig({\n",
    "    'project': project,\n",
    "    'random_seed': 377778,\n",
    "    'start_times': start_times,\n",
    "    'nsteps': nsteps,          # number of forecasting steps\n",
    "    'nensemble': nensemble,       # ensemble size per checkpoint\n",
    "    'batch_size': batch_size,      # inference batch size\n",
    "\n",
    "    'forecast_model': {\n",
    "        'architecture': 'earth2studio.models.px.SFNO',   # forecast model class\n",
    "        'package': model_packages,\n",
    "        'max_num_checkpoints': max_num_checkpoints  # max number of checkpoints which will be used\n",
    "    },\n",
    "\n",
    "    'data_source': {\n",
    "        '_target_': 'earth2studio.data.GFS'  # data source class\n",
    "    },\n",
    "\n",
    "    'cyclone_tracking': {\n",
    "        'out_dir': out_dir\n",
    "    },\n",
    "\n",
    "    'file_output': {\n",
    "        'path': out_dir,       # directory to which outfiles are written\n",
    "        'output_vars': output_vars,\n",
    "        'thread_io': False,      # write out in separate thread\n",
    "        'format': {              # io backend class\n",
    "            '_target_': 'earth2studio.io.NetCDF4Backend',\n",
    "            '_partial_': True,\n",
    "            'backend_kwargs': {\n",
    "                'mode': 'w',\n",
    "                'diskless': False,\n",
    "                'persist': False,\n",
    "                'chunks': {\n",
    "                    'ensemble': 1,\n",
    "                    'time': 1,\n",
    "                    'lead_time': 1\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialise the inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from ensemble_utilities import EnsembleBase\n",
    "from reproduce_utilities import create_base_seed_string\n",
    "\n",
    "from utilities import (\n",
    "    initialise,\n",
    "    initialise_output,\n",
    "    store_tracks,\n",
    "    update_model_dict,\n",
    "    write_to_disk,\n",
    ")\n",
    "\n",
    "(\n",
    "    ensemble_configs,\n",
    "    model_dict,\n",
    "    dx_model_dict,\n",
    "    cyclone_tracking,\n",
    "    data,\n",
    "    output_coords_dict,\n",
    "    base_random_seed,\n",
    "    all_tracks_dict,\n",
    "    _, _\n",
    ") = initialise(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialisation provides us with a number of objects that will be used throughout the inference. Let's have a closer look at two of them:\n",
    "\n",
    "First, we explore the ensemble configs, a list of tuples containing information on which model package and initial conditions to use. Additionally, it contains ensemble indices and batch IDs assuring these are unique across all processes. Let's have a look at the content and explore how many cases we will run, depending on number of ensemble members, batch size, number of checkpoints and initial conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii, (pkg, ic, ens_idx, batch_ids_produce) in enumerate(ensemble_configs):\n",
    "    print(f'ensemble config {ii+1} of {len(ensemble_configs)}:')\n",
    "    print(f'    package: {pkg}')\n",
    "    print(f'    initial condition: {ic}')\n",
    "    print(f'    ensemble index of first member: {ens_idx}')\n",
    "    print(f'    batch ids to produce: {batch_ids_produce}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the inference is parallelised across enssemble config elements, hence across IC-package pairs. As a result, you cannot use more GPUs than number of ICs multiplied by number of checkpoints. If more GPUs are available, they remain idle.\n",
    "\n",
    "The model dict includes information about the model class and which model weights are currently loaded. In addtion, it also holds a pointer to the model loaded to the GPU.\n",
    "During infernce, the model dict gets updated according to the information provided in the ensemble config. Let's have a look at its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "print(colored('The model class is:', attrs=['bold']))\n",
    "print(model_dict['class'], '\\n')\n",
    "\n",
    "print(colored('The model package (weights), which is currently loaded:', attrs=['bold']))\n",
    "print(model_dict['package'], '\\n')\n",
    "\n",
    "print(colored('The fully initialised model is provided in:', attrs=['bold']))\n",
    "print(model_dict['model'].parameters, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final piece which is missing before we can run the inference is assembling the HENS perturbation. For this, we need to provide:\n",
    "- a skill file, which contains the deterministic skill of the forecast model (**Note**: provide links to download skill file, best in intro)\n",
    "- the variable to perturb in the seeding step of the bred vector perturbation\n",
    "- the number of integration steps for breeding the noise vector\n",
    "- the noise amplification, by which the noise vector is scaled\n",
    "\n",
    "With this information, we can now assemble the HENS perturbation using CorrelatedSphericalGaussian as seeding perturbation and HemisphericCentredBredVector as bred vector perturbation. To see how it is aseembled form basic blocks porvided in e2studio, have a look into `11_hens/hens_perturbation.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for perturbation\n",
    "skill_path = \"/media/mkoch/9ee63bf8-5a14-4872-86f2-7f16b120269b/hens_data/hens_checkpoints/d2m_sfno_linear_74chq_sc2_layers8_edim620_wstgl2-epoch70_seed16.nc\"\n",
    "noise_amplification = 0.35\n",
    "perturbed_var = [\"z500\"]\n",
    "integration_steps = 3\n",
    "\n",
    "from hens_perturbation import HENSPerturbation\n",
    "\n",
    "from numpy import ndarray, datetime64\n",
    "\n",
    "from earth2studio.data import DataSource\n",
    "from earth2studio.models.px import PrognosticModel\n",
    "from earth2studio.perturbation import Perturbation\n",
    "\n",
    "def initialise_perturbation(\n",
    "    model: PrognosticModel,\n",
    "    data: DataSource,\n",
    "    start_time: ndarray[datetime64],\n",
    ") -> Perturbation:\n",
    "    perturbation = HENSPerturbation(\n",
    "        model=model,\n",
    "        data=data,\n",
    "        start_time=start_time,\n",
    "        skill_path=skill_path,\n",
    "        noise_amplification=noise_amplification,\n",
    "        perturbed_var=perturbed_var,\n",
    "        integration_steps=integration_steps\n",
    "    )\n",
    "\n",
    "    return perturbation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now bring everyhting together:\n",
    "- loop over ensemble configs\n",
    "- update model dict (if package has changed)\n",
    "- initialise output\n",
    "- initialise perturbation (as ICs might have changed)\n",
    "- run inference, where all ensemble members are produced\n",
    "- write to disk\n",
    "\n",
    "\n",
    "Now we will bring all components together to execute the ensemble forecasting process:\n",
    "\n",
    "- iterate through each ensemble configuration, which contains the necessary parameters for generating individual ensemble members. \n",
    "- at each iteration, update the model dictionary whenever a new package is encountered, ensuring the correct model weights are loaded.\n",
    "- initialise the output object and set up the perturbation method, taking into account any changes in the initial conditions.\n",
    "- initialise the perturbation method with updated IC and checkpoint\n",
    "- initialise the inference pipeline with updated IC and checkpoint\n",
    "- run inference, which generates all ensemble members according to the specified configuration.\n",
    "- write the results to disk, ensuring that all forecast data and associated metadata are properly stored for subsequent analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over ensemble configs\n",
    "for pkg, ic, ens_idx, batch_ids_produce in ensemble_configs:\n",
    "    # create seed base string required for reproducibility of individual batches\n",
    "    base_seed_string = create_base_seed_string(pkg, ic, base_random_seed)\n",
    "\n",
    "    # load new weights if necessary\n",
    "    model_dict = update_model_dict(model_dict, pkg)\n",
    "\n",
    "    # create new io object\n",
    "    io_dict = initialise_output(cfg, ic, model_dict, output_coords_dict)\n",
    "\n",
    "    # initialise perturbation with updated IC and checkpoint\n",
    "    perturbation = initialise_perturbation(\n",
    "        model=model_dict[\"model\"], data=data, start_time=ic\n",
    "    )\n",
    "\n",
    "    # initialise inference pipeline with updated IC and checkpoint\n",
    "    run_hens = EnsembleBase(\n",
    "        time=[ic],\n",
    "        nsteps=cfg.nsteps,\n",
    "        nensemble=cfg.nensemble,\n",
    "        prognostic=model_dict[\"model\"],\n",
    "        data=data,\n",
    "        io_dict=io_dict,\n",
    "        perturbation=perturbation,\n",
    "        output_coords_dict=output_coords_dict,\n",
    "        dx_model_dict=dx_model_dict,\n",
    "        cyclone_tracking=cyclone_tracking,\n",
    "        batch_size=cfg.batch_size,\n",
    "        ensemble_idx_base=ens_idx,\n",
    "        batch_ids_produce=batch_ids_produce,\n",
    "        base_seed_string=base_seed_string,\n",
    "    )\n",
    "\n",
    "    # run inference\n",
    "    df_tracks_dict, io_dict = run_hens()\n",
    "\n",
    "    # store tracks\n",
    "    for k, v in df_tracks_dict.items():\n",
    "        v[\"ic\"] = pd.to_datetime(ic)\n",
    "        all_tracks_dict[k].append(v)\n",
    "\n",
    "    # if in-memory flavour of io backend was chosen, write content to disk now\n",
    "    if io_dict:\n",
    "        _, _ = write_to_disk(\n",
    "            cfg,\n",
    "            ic,\n",
    "            model_dict,\n",
    "            io_dict,\n",
    "            None,\n",
    "            None,\n",
    "        )\n",
    "\n",
    "# write cyclone tracks to disk\n",
    "if \"cyclone_tracking\" in cfg:\n",
    "    for area_name, all_tracks in all_tracks_dict.items():\n",
    "        store_tracks(area_name, all_tracks, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the ensemble generation process, the results are stored in the output directory. This directory contains both the forecast fields and the cyclone track data. A seperate output file is created for each checkpoint-IC pair, so if you haven't changed the configs, there should be two NETCDF files with field data and one CSV file with the tracks.\n",
    "\n",
    "The field data includes:\n",
    "- 4 ensemble members\n",
    "- 1 initial condition (time)\n",
    "- 17 lead times for each forecast field (IC + 16 forecast steps)\n",
    "\n",
    "The track data includes detailed information about the storm's position, intensity, and other relevant meteorological parameters at each time step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from plotting.fork_n_spoon import extract_tracks_from_csv\n",
    "\n",
    "ds = xr.load_dataset('outputs/global/helene_2024-09-24T12_pkg_seed102.nc')\n",
    "display(ds)\n",
    "\n",
    "tracks = pd.read_csv('outputs/global/helene_tracks_rank_000.csv', sep=',')\n",
    "print('tracks columns:')\n",
    "print(list(tracks.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will now visualise these results. First, let us have a quick look at the global field after one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['t2m'].isel(ensemble=0, lead_time=4, time=0).plot(figsize=(16, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's look at the Hurricane fiedl and draw some tracks. To do so, we start by extracting the tracks from the CSV file. For this, we choose tracks that originate close to the actual position of Hurricane Helene and include at least 4 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting.fork_n_spoon import ibtracs_helene\n",
    "\n",
    "# tracks = pd.read_csv('outputs/global/helene_tracks_rank_000.csv', sep=',')\n",
    "track_list, _ = extract_tracks_from_csv('outputs/global/helene_tracks_rank_000.csv',\n",
    "                                ic=start_times[0],\n",
    "                                tc_centres=ibtracs_helene(),\n",
    "                                max_dist=2.5,\n",
    "                                min_len=4,\n",
    "                                max_stp=nsteps)\n",
    "\n",
    "print(f'found {len(track_list)} tracks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's focus on the Gulf of Mexico and plot the tracks of Hurricane Helene. You can select the enselmble member and the variable you want to show by editing the first lines in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'u10m'\n",
    "ensemble_member = 1\n",
    "\n",
    "max_frames = 17 # maximum number of frames to plot\n",
    "scale = 1\n",
    "\n",
    "lat_min, lat_max = 10, 40\n",
    "lon_min, lon_max = 250, 300\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "\n",
    "from plotting.fork_n_spoon import make_figure, make_frame\n",
    "\n",
    "dx = scale*.25\n",
    "\n",
    "countries = cfeature.NaturalEarthFeature(\n",
    "    category=\"cultural\",\n",
    "    name=\"admin_0_countries\",\n",
    "    scale=\"110m\",\n",
    "    facecolor=\"none\",\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "\n",
    "# extract region of interest\n",
    "reg_ds = ds.sel(lat=list(np.arange(lat_min, lat_max, dx)),\n",
    "                lon=list(np.arange(lon_min, lon_max, dx)))\n",
    "\n",
    "time_str = 'lead time:'\n",
    "projection=ccrs.PlateCarree()\n",
    "var_ds = reg_ds[variable] # np.sqrt(np.square(reg_ds.u10m) + np.square(reg_ds.v10m))\n",
    "\n",
    "min_val = float(np.min(var_ds[ensemble_member,0,:,:,:]))\n",
    "max_val = float(np.max(var_ds[ensemble_member,0,:,:,:]))\n",
    "\n",
    "# make animation\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "fig, ax = make_figure(projection=ccrs.PlateCarree())\n",
    "\n",
    "_make_frame = make_frame(fig, ax, var_ds, ensemble_member, track_list, max_frames, min_val, max_val, projection, reg_ds, time_str)\n",
    "\n",
    "def animate(frame):\n",
    "    return _make_frame(frame)\n",
    "\n",
    "def first_frame():\n",
    "    return _make_frame(-1)\n",
    "\n",
    "ani = animation.FuncAnimation(fig,\n",
    "                              animate,\n",
    "                              min(max_frames, var_ds.shape[2]),\n",
    "                              init_func=first_frame,\n",
    "                              blit=False,\n",
    "                              repeat=False,\n",
    "                              interval=.1)\n",
    "plt.close('all')\n",
    "display(ani)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let us draw all the tracks from all eight genereted ensemble members:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=projection)\n",
    "\n",
    "ax.add_feature(cfeature.COASTLINE,lw=.5)\n",
    "ax.add_feature(cfeature.RIVERS,lw=.5)\n",
    "ax.add_feature(cfeature.OCEAN)\n",
    "ax.add_feature(cfeature.LAND)\n",
    "\n",
    "lon_formatter = LongitudeFormatter(zero_direction_label=False)\n",
    "lat_formatter = LatitudeFormatter()\n",
    "ax.xaxis.set_major_formatter(lon_formatter)\n",
    "ax.yaxis.set_major_formatter(lat_formatter)\n",
    "\n",
    "# Plot the line in white\n",
    "for track in track_list:\n",
    "    ax.plot(track['lon']-360, track['lat'],\n",
    "            color='crimson', linewidth=2, alpha=.4)\n",
    "\n",
    "ax.set_extent([lon_min, lon_max, lat_min, lat_max])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
